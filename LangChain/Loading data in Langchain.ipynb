{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1379f614",
   "metadata": {},
   "source": [
    "### Load pdfs as Text\n",
    "using pypdfloader inside Langcahin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebbb3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"transformers_survey.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27fdcf1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the pdf is a 22 page long research article\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "318b8f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_page = pages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c153b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Open 3 (2022) 111â€“132\n",
      "112T. Lin et al.\n",
      "Fig. 1. Overview of vanilla Transformer architecture.\n",
      "2. Background\n",
      "2.1. Vanilla Transformer\n",
      "The vanilla Transformer (Vaswani et al., 2017) is a sequence-to-\n",
      "sequence model and consists of an encoder and a decoder, each of\n",
      "which is a stack of ğ¿identical blocks. Each encoder block is mainly\n",
      "composed of a multi-head self-attention module and a position-wise\n",
      "feed-forward network (FFN). For building a deeper model, a residual\n",
      "connection (He et al., 2016) is employed around each module, followed\n",
      "by Layer Normalization (Ba et al., 2016) module. Compared to the\n",
      "encoder blocks, decoder blocks additionally insert cross-attention mod-\n",
      "ules between the multi-head self-attention modules and the position-\n",
      "wise FFNs. Furthermore, the self-attention modules in the decoder\n",
      "are adapted to prevent each position from attending to subsequent\n",
      "positions. The overall architecture of the vanilla Transformer is shown\n",
      "in Fig. 1.\n",
      "In the following subsection, we shall introduce the key modules of\n",
      "the vanilla Transformer.\n",
      "2.1.1. Attention modules\n",
      "Transformer adopts attention mechanism with Queryâ€“Keyâ€“Value\n",
      "(QKV) model. Given the packed matrix representations of queries ğâˆˆ\n",
      "Rğ‘Ã—ğ·ğ‘˜, keys ğŠâˆˆRğ‘€Ã—ğ·ğ‘˜, and values ğ•âˆˆRğ‘€Ã—ğ·ğ‘£, the scaled dot-product\n",
      "attention used by Transformer is given by1\n",
      "Attention( ğ,ğŠ,ğ•) = softmax(\n",
      "ğğŠâŠ¤\n",
      "âˆš\n",
      "ğ·ğ‘˜)\n",
      "ğ•=ğ€ğ•, (1)\n",
      "1If not stated otherwise, we use row-major notations throughout this survey\n",
      "(e.g., theğ‘–th row in ğis the query ğªğ‘–) and all the vectors are row vectors by\n",
      "default.whereğ‘andğ‘€denote the lengths of queries and keys (or values);\n",
      "ğ·ğ‘˜andğ·ğ‘£denote the dimensions of keys (or queries) and values; ğ€=\n",
      "softmax(\n",
      "ğğŠâŠ¤\n",
      "âˆš\n",
      "ğ·ğ‘˜)\n",
      "is often called attention matrix ; softmax is applied in a\n",
      "row-wise manner. The dot-products of queries and keys are divided byâˆš\n",
      "ğ·ğ‘˜to alleviate gradient vanishing problem of the softmax function.\n",
      "Instead of simply applying a single attention function, Transformer\n",
      "uses multi-head attention, where the ğ·ğ‘š-dimensional original queries,\n",
      "keys and values are projected into ğ·ğ‘˜,ğ·ğ‘˜andğ·ğ‘£dimensions, re-\n",
      "spectively, with ğ»different sets of learned projections. For each of\n",
      "the projected queries, keys and values, and output is computed with\n",
      "attention according to Eq. (1). The model then concatenates all the\n",
      "outputs and projects them back to a ğ·ğ‘š-dimensional representation.\n",
      "MultiHeadAttn( ğ,ğŠ,ğ•) = Concat(head1,â€¦,headğ»)ğ–ğ‘‚, (2)\n",
      "where headğ‘–= Attention( ğğ–ğ‘„\n",
      "ğ‘–,ğŠğ–ğ¾\n",
      "ğ‘–,ğ•ğ–ğ‘‰\n",
      "ğ‘–). (3)\n",
      "In Transformer, there are three types of attention in terms of the\n",
      "source of queries and keyâ€“value pairs:\n",
      "â€¢Self-attention . In Transformer encoder, we set ğ=ğŠ=ğ•=ğ—in\n",
      "Eq. (2), where ğ—is the outputs of the previous layer.\n",
      "â€¢Masked Self-attention . In the Transformer decoder, the self-attention\n",
      "is restricted such that queries at each position can only attend to\n",
      "all keyâ€“value pairs up to and including that position. To enable\n",
      "parallel training, this is typically done by applying a mask func-\n",
      "tion to the unnormalized attention matrix Ì‚ğ€= exp(ğğŠâŠ¤\n",
      "âˆš\n",
      "ğ·ğ‘˜), where\n",
      "the illegal positions are masked out by setting Ì‚ğ´ğ‘–ğ‘—= âˆ’âˆ ifğ‘–<ğ‘—.\n",
      "This kind of self-attention is often referred to as autoregressive or\n",
      "causal attention.2\n",
      "2This term seems to be borrowed from the causal system , where the output\n",
      "depends on past and current inputs but not future inputs.\n"
     ]
    }
   ],
   "source": [
    "print(single_page.page_content[0:5000])\n",
    "#we priented up to 5000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6012f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'transformers_survey.pdf', 'page': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_page.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d782b31",
   "metadata": {},
   "source": [
    "### Load Website data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a396950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "Abrar Fahim        \n",
      "\n",
      "Abrar Fahim\n",
      " Data Scientist | Former KTP Associate \n",
      "Github Linkedin   Download CV  \n",
      "\n",
      "Highlights\n",
      "\n",
      "          - Data Scientist with experience in the HealthCare and FinTech industry \n",
      "          - Endorsed by UKRI, obtained a Global Talent Visa\n",
      "          - Solid Computer Science Background \n",
      "          - Experience in both Industry and Academic setup  \n",
      "\n",
      "Education Msc in Data Science and Analytics\n",
      "Bsc in Computer Science and Engineering Email abrar\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://www.abrarfahim.co.uk/\")\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:500].replace(\"\\n\\n\\n\", \" \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
