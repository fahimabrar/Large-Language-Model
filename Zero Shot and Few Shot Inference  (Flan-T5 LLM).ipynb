{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbfa354e",
   "metadata": {},
   "source": [
    "**In this notebook,**\n",
    "<br>we will use flan-t5 model for question answering. the idea is as follwoing,\n",
    "\n",
    "- we will use the model to perform multiple choise question task with no example (**zero shot**)\n",
    "- the model gives output with only one word, but we need more explanation.\n",
    "- we will use **few shot** learning to get the intended style of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4794aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717f7ca",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "FLAN-T5 was released in the paper Scaling Instruction-Finetuned Language Models <br>\n",
    "it is an enhanced version of T5 that has been finetuned in a mixture of tasks.\n",
    "\n",
    "huggingface repo >> https://huggingface.co/google/flan-t5-base\n",
    "\n",
    "we used torch_dtype bfloat16, developed by google brain as it is more computation efficient, details >> https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "229c5356",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef7b2e",
   "metadata": {},
   "source": [
    "### Load the dataset (CommonsenseQA)\n",
    "***Dataset Summary***\n",
    "\n",
    "CommonsenseQA is a new multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers. It contains 12,102 questions with one correct answer and four distractor answers. \n",
    "\n",
    "the dataset have follwing columns:\n",
    "**id, question, question_concept, choices, text and answerKey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a9f856e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset commonsense_qa (C:/Users/abrar/.cache/huggingface/datasets/commonsense_qa/default/1.0.0/28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0b18d368aa4a47844662c283c1d440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '075e483d21c29a511267ef62bedc0461',\n",
       " 'question': 'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?',\n",
       " 'question_concept': 'punishing',\n",
       " 'choices': {'label': ['A', 'B', 'C', 'D', 'E'],\n",
       "  'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']},\n",
       " 'answerKey': 'A'}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"commonsense_qa\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd79b40",
   "metadata": {},
   "source": [
    "### Function for generating prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ded18003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will load option for a single question\n",
    "#the variable data_split can be one of the follwing: train, test, or validation.\n",
    "#that means we want data from training, test or validation sets\n",
    "\n",
    "def options(data_split,index):\n",
    "    text = ''\n",
    "    for i in range(len(dataset[data_split][index]['choices']['label'])):\n",
    "        text = text + \"-\"+ dataset['train'][index]['choices']['text'][i] + '\\n'\n",
    "    return text\n",
    "\n",
    "#prompt_generator generates promp\n",
    "#the following cell wil show how a single prompt is generated\n",
    "def prompt_generator(data_split, index):\n",
    "    question = dataset[data_split][index]['question']\n",
    "    text = options(data_split,index)\n",
    "    answer = dataset[data_split][index]['answerKey']\n",
    "    prompt = f\"\"\"{question}\\n{text}\\nanswer:\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59205945",
   "metadata": {},
   "source": [
    "**let's see how a single prompt will look like,**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cee95bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT>>>>>>>>\n",
      "What are you hoping to achieve when talking to someone?\n",
      "-communicating with others\n",
      "-hurt feelings\n",
      "-dry mouth\n",
      "-intimacy\n",
      "-changing behavior\n",
      "\n",
      "answer:\n"
     ]
    }
   ],
   "source": [
    "print('PROMPT>>>>>>>>')\n",
    "print(prompt_generator('train', 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0c0516da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT\n",
      "---------------------------------------------------\n",
      "What is someone who gets angry after getting drunk likely to participate in?\n",
      "-violence\n",
      "-falling down\n",
      "-vomiting\n",
      "-vomiting\n",
      "-lower standards\n",
      "\n",
      "answer:\n",
      "---------------------------------------------------\n",
      "\n",
      "MODEL GENERATED>> \u001b[32mviolence\u001b[0m\n",
      "Actual answer: A\n",
      "\u001b[31mN:B: options are as follows: first option is A, then B, C and so on.. so the answer is correct\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "index = 91\n",
    "data_split = 'train'\n",
    "\n",
    "#the folling line will use prompt generator function for generating prompt\n",
    "#it will generate prompot from training dataset, for index 91\n",
    "prompt = prompt_generator(data_split, index)\n",
    "\n",
    "\n",
    "#inside tokenizer we give prompt as input, it will tokenize the inputs into smaller chunk and represent in numbers\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "#output is the generated response from the llm, look here original_model refers to flan-t5-base model\n",
    "#the original_model fed with input['input_ids'], that are tokenized numeric representation of the prompt\n",
    "#you can pring inputs['input_ids'] to see what inside them\n",
    "#the generated output is within 50 words, as max_new_tokens set to 50\n",
    "#skip_special_tokens = True, that means we donot want to sese noises (e.g. a special token [EOS] represents, end of sentences) \n",
    "\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "print('PROMPT')\n",
    "print('---------------------------------------------------')\n",
    "print(prompt)\n",
    "print('---------------------------------------------------')\n",
    "print()\n",
    "print('MODEL GENERATED>> ', end = \"\")\n",
    "print(colored(output, 'green'))\n",
    "print('Actual answer:', dataset['train'][index]['answerKey'])\n",
    "print(colored('N:B: options are as follows: first option is A, then B, C and so on.. so the answer is correct', 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c27c2",
   "metadata": {},
   "source": [
    "If we look at the output we can see the model choosed one option from the options. But we need the model to explain the result in few sentences. A few shot inference can be helpful for the model to generate the intended output style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0990e4c",
   "metadata": {},
   "source": [
    "### Example for few shot inference\n",
    "\n",
    "**What is a few shot inference?** <br>\n",
    "\n",
    "-- In few shot inference we show some examples to the model. So, the model understands what types of output we are expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "05156a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"\n",
    "What is necessary for learning by yourself?\n",
    "- exposure\n",
    "- patience\n",
    "- study\n",
    "- cooperation\n",
    "- thought\n",
    "\n",
    "answer: study. learning by yourself opens enormous opportunity to gain knowledge. A person should study by him or herself to learn alone. When there is no one to help, you should learn by yourself.\\n\n",
    "\n",
    "What emotion does getting paid lead to?\n",
    "- sorrow\n",
    "- paying bills\n",
    "- happiness\n",
    "- frustration\n",
    "- spending money\n",
    "\n",
    "answer: happiness. as happiness is a positive emotion and when someone get paid they become happy. Money sometimes is associated to happiness, although money cannot buy happiness. But things that bring happiness can be bought by money.\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e93ea7",
   "metadata": {},
   "source": [
    "### Output generation with few shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "998c068a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT\n",
      "---------------------------------------------------\n",
      "\n",
      "What is necessary for learning by yourself?\n",
      "- exposure\n",
      "- patience\n",
      "- study\n",
      "- cooperation\n",
      "- thought\n",
      "\n",
      "answer: study. learning by yourself opens enormous opportunity to gain knowledge. A person should study by him or herself to learn alone. When there is no one to help, you should learn by yourself.\n",
      "\n",
      "\n",
      "What emotion does getting paid lead to?\n",
      "- sorrow\n",
      "- paying bills\n",
      "- happiness\n",
      "- frustration\n",
      "- spending money\n",
      "\n",
      "answer: happiness. as happiness is a positive emotion and when someone get paid they become happy. Money sometimes is associated to happiness, although money cannot buy happiness. But things that bring happiness can be bought by money.\n",
      "\n",
      "What is someone who gets angry after getting drunk likely to participate in?\n",
      "-violence\n",
      "-falling down\n",
      "-vomiting\n",
      "-vomiting\n",
      "-lower standards\n",
      "\n",
      "answer:\n",
      "---------------------------------------------------\n",
      "\n",
      "MODEL GENERATED>> \u001b[32mviolence. violence is a violent activity. Someone who gets angry after getting drunk is likely to participate in violence.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "index = 91\n",
    "data_split = 'train'\n",
    "prompt =  example + prompt_generator(data_split, index)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print('PROMPT')\n",
    "print('---------------------------------------------------')\n",
    "print(prompt)\n",
    "print('---------------------------------------------------')\n",
    "print()\n",
    "print('MODEL GENERATED>> ', end = \"\")\n",
    "print(colored(output, 'green'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
