{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d53417",
   "metadata": {},
   "source": [
    "# Fine Tuning a LLM for Legal Documents (flan-t5-small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b2269d",
   "metadata": {},
   "source": [
    "**In this notebook,** \n",
    "\n",
    "we will fine tune flan t5 small llm model for legal documents summarization\n",
    "\n",
    "for summarization we will use [legal_summarization](https://huggingface.co/datasets/egalize/legal_summarization) dataset.\n",
    "The dataset contains mainly terms and conditions, privacy, legal notice for tech products and their summary. \n",
    "\n",
    "- first we will use flan-t5-base model for summarization of the legal documents\n",
    "  we will score the model's performance against the reference summary provided with the dataset\n",
    "  then we will calculate the rouge_score\n",
    "\n",
    "- then we will fine tune the model with the training dataset provided with the dataset\n",
    "  we will calculate the rouge_score for the fine tuned model\n",
    "\n",
    "- and then compare between them, if the model's performacne increased after a fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5238bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "#the library from huggingface, useful for directly download any huggingface datasets\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from termcolor import colored\n",
    "#for printing in color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c094c",
   "metadata": {},
   "source": [
    "###  Load the Model and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb7d4fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/abrar/.cache/huggingface/datasets/egalize___csv/egalize--legal_summarization-454cda284ab7f119/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6faa66aff3b84fd49a930402b1a5362a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['original_text', 'reference_summary'],\n",
       "        num_rows: 356\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['original_text', 'reference_summary'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name='google/flan-t5-small'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "# the model is loaded from huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# load the tokenizer to tokenize the corpus\n",
    "\n",
    "dataset_name = 'egalize/legal_summarization'\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffec55d",
   "metadata": {},
   "source": [
    "#### Number of trainable parameters in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c12e69a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 76961152\n",
      "all model parameters: 76961152\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_params}\\nall model parameters: {all_params}\\npercentage of trainable model parameters: {100 * trainable_params / all_params:.2f}%\"\n",
    "\n",
    "print(trainable_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f4df2c",
   "metadata": {},
   "source": [
    "### Functions for prompt generation and getting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b28042d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_generator(text):\n",
    "    prompt = f'''Briefly summarize this paragraph:\\n{text}\\nSummary:'''\n",
    "    #print(colored('PROMPT>>', 'red'))\n",
    "    #print(colored('*******************************', 'red'))\n",
    "    #print(prompt)\n",
    "    #print(colored('*******************************\\n', 'red'))\n",
    "    return prompt\n",
    " \n",
    "def get_output(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "            model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=25,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return output\n",
    "\n",
    "## in get_output function, in input we tokenize the prompt itself,\n",
    "## then in output varialbe we get the models response, it is done with following steps,\n",
    "# - model.generate() function takes input['input_ids'] that is a numeric representation of the prompot. \n",
    "# -  max_new token set to 25, that means the model generates 25 words, \n",
    "# - skip_speical_token set to false, so no special token will be shown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88542852",
   "metadata": {},
   "source": [
    "### Generate prompt and see a output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9adbe6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPROMPT>>\u001b[0m\n",
      "Briefly summarize this paragraph:\n",
      "for api clients that use their own avatar naming system in place of the user s google identity then you must make clear to users that their gameplay information will still be submitted to google and associated with their google identity and viewable within different google products.\n",
      "Summary:\n",
      "\n",
      "\u001b[32mMODEL GENERATED>>\u001b[0m api clients must make clear that their gameplay information will still be submitted to google and associated with their google identity and viewable\n",
      "\u001b[32mActual Summary>>\u001b[0m if using avatars usernames tell the user that their g identity will still be used by google.\n"
     ]
    }
   ],
   "source": [
    "article = dataset['train'][1]['original_text']\n",
    "summary = dataset['train'][1]['reference_summary']\n",
    "print(colored('PROMPT>>', 'red'))\n",
    "print(prompt_generator(article))\n",
    "print()\n",
    "\n",
    "prompt = prompt_generator(article)\n",
    "print(colored('MODEL GENERATED>>', 'green'), get_output(prompt))\n",
    "print(colored('Actual Summary>>', 'green'), summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37cb1c",
   "metadata": {},
   "source": [
    "### Calculate ROUGE SCORE\n",
    "\n",
    "generally speaking, rouge is a measure of the similarity between the generated summary and the acutal summary. the scale is between 0 and 1. The more the better the score. \n",
    "\n",
    "note that we have 90 summary for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8643fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1064 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.15884607716620358,\n",
       " 'rouge2': 0.04355273527390338,\n",
       " 'rougeL': 0.13169214077239494,\n",
       " 'rougeLsum': 0.1298386933909495}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "model_generated_summary = []\n",
    "for i in range(len(dataset['test'])):\n",
    "    print(i, end = ' ')\n",
    "    article = dataset['test'][i]['original_text']\n",
    "    prompt = prompt_generator(article)\n",
    "    model_generated_summary.append(get_output(prompt))\n",
    "    \n",
    "#this function will get all the generated_output by flan-t5-small output in a list\n",
    "# for all the test data\n",
    "\n",
    "\n",
    "result = rouge.compute(\n",
    "    predictions=model_generated_summary,\n",
    "    references=dataset['test']['reference_summary'],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce09339",
   "metadata": {},
   "source": [
    "### Fine Tuning Flan-T5-Small model for legal summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82879b",
   "metadata": {},
   "source": [
    "Transform the dataset for the fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a9d08a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\abrar\\.cache\\huggingface\\datasets\\egalize___csv\\egalize--legal_summarization-454cda284ab7f119\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-1c80317fa3b1799d.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\abrar\\.cache\\huggingface\\datasets\\egalize___csv\\egalize--legal_summarization-454cda284ab7f119\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-bdd640fb06671ad1.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize(example):\n",
    "    start_prompt = 'Briefly summarize this paragraph:\\n'\n",
    "    end_prompt = '\\nSummary:'\n",
    "    prompt = [start_prompt + text + end_prompt for text in example[\"original_text\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"reference_summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    return example\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "#the data automatically split into train and test\n",
    "# to see how the new dataset looks like you can print some of the rows \n",
    "\n",
    "#uncomment the following line\n",
    "#print(tokenized_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7edfeca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the hyperparameters\n",
    "\n",
    "\n",
    "output_dir = f'./legal-summary-training'\n",
    "#define the output folder\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainig wil be begin\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff85fd",
   "metadata": {},
   "source": [
    "####  To save and load the model >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8b7a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./tuned_model\")\n",
    "\n",
    "# I trained a modle on google colab pro with more epoch\n",
    "# am loding that model for a better rouge score, and better understanding\n",
    "\n",
    "tuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"./tuned_model_colab\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "def get_tuned_model_output(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "            tuned_model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=25,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53852938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPROMPT>>>\u001b[0m\n",
      "Briefly summarize this paragraph:\n",
      "for api clients that use their own avatar naming system in place of the user s google identity then you must make clear to users that their gameplay information will still be submitted to google and associated with their google identity and viewable within different google products.\n",
      "Summary:\n",
      "\n",
      "\u001b[32mTUNED MODEL GENERATED>>\u001b[0m api clients must make clear that their gameplay information will still be submitted to google and associated with their google identity and viewable\n",
      "\u001b[32mActual Summary>>\u001b[0m if using avatars usernames tell the user that their g identity will still be used by google.\n"
     ]
    }
   ],
   "source": [
    "print(colored('PROMPT>>>', 'red'))\n",
    "print(prompt_generator(article))\n",
    "\n",
    "prompt = prompt_generator(article)\n",
    "print()\n",
    "print(colored('TUNED MODEL GENERATED>>', 'green'), get_tuned_model_output(prompt))\n",
    "\n",
    "print(colored('Actual Summary>>', 'green'), summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53888f0e",
   "metadata": {},
   "source": [
    "### Fine Tuned Model Rouge Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53b92f01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.1643423498997253,\n",
       " 'rouge2': 0.043617406432127116,\n",
       " 'rougeL': 0.13482787082209935,\n",
       " 'rougeLsum': 0.13476832202447428}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model_generated_summary = []\n",
    "for i in range(len(dataset['test'])):\n",
    "    print(i, end = ' ')\n",
    "    article = dataset['test'][i]['original_text']\n",
    "    prompt = prompt_generator(article)\n",
    "    tuned_model_generated_summary.append(get_tuned_model_output(prompt))\n",
    "    \n",
    "    \n",
    "result = rouge.compute(\n",
    "    predictions=tuned_model_generated_summary,\n",
    "    references=dataset['test']['reference_summary'],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5efa3",
   "metadata": {},
   "source": [
    "### Analysis and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c0e0d",
   "metadata": {},
   "source": [
    "Before Fine Tuning >> Rouge 1 \n",
    "\n",
    "|Rouge | Before   | After |\n",
    "|---| ---| --- |\n",
    "|rouge1| 0.15  | 0.16   |\n",
    "|rouge2| 0.04 | 0.04    |\n",
    "|rougeL| 0.13    | 0.13 |\n",
    "|rougeLsum| 0.12    | 0.13 |\n",
    "\n",
    "<br><br>\n",
    "\n",
    "we trained the model in a small dataset. the dataest only have 356 training data. The main objective of this notebook is not getting a sophisticated output, rathet to learn how to fine tune a model for a specific task. due to lack of data and computing resource we may appeciate the process rather than the outcome\n",
    "\n",
    "thank you\n",
    "\n",
    "#### Other notebooks on LLM [large Language Models](https://github.com/fahimabrar/Large-Language-Model)\n",
    "#### Find me on, [Linkedin](https://www.linkedin.com/in/abrar-fahim/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
